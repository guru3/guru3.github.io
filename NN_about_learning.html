<!DOCTYPE HTML>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Deep Learning on properly and randomly labelled data</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />    
	<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
				<!-- Note: The "styleN" class below should match that of the banner element. -->
					<header id="header" style='background-color:inherit' class="alt style2;">
						<a href="index.html" class="logo"><strong>guru3</strong></a>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<ul class="links">
							<li><a href="index.html">Home</a></li>
							<li><a href="projects.html">Projects</a></li>
							<li><a href="thoughts.html">Thoughts</a></li>
							<li><a href="about.html">About</a></li>
						</ul>
					</nav>

				<!-- Banner -->
				<!-- Note: The "styleN" class below should match that of the header element. -->
					<section id="banner" class="style2">
						<div class="inner">
							<span class="image">
								<img src="images/project1/mnist.png" alt="" />
							</span>
							<header class="major" style='background:linear-gradient(to left, #00ffff 0%, #000066 97%);border-radius:25px;opacity:0.8'>
								<h1>Deep Learning on correctly and randomly labelled data</h1>
							</header>
						</div>
					</section>

				<!-- Main -->
					<div id="main" style='background:black;'>

						<!-- One -->
							<section id="one">
								<div class="inner">
									<p>Neural network in its architecture does not assume anything related to structure of data. It is simply a pattern finding machine which takes in data and churns out its corresponding mapping and learns to make this mapping as close for training data as possible.
									So if we provide neural network with randomly labelled data as well will it perform as good? Or will neural network not able to achieve as good accuracy as the one trained on correctly labelled data?</p>
									</br>

									<p>Before we go ahead, you can find the notebook link <a href="https://github.com/guru3/ML-Insights/blob/master/3.%20Deep%20Learning%20on%20properly%20and%20randomly%20labelled%20data.ipynb"><span style='color:red'>here</span></a>.</br>

									<span style='font-weight:bold'>We will using MNIST data</span> and write a neural network to recognize the digits looking at the images. Keras does support this dataset using  very simple call.
									
									<div style="background:black;overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;">
										<pre style="margin: 0; line-height: 125%">
<span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">keras.datasets</span> <span style="color: #008800; font-weight: bold">import</span> mnist
(x_train_raw, y_train_raw), (x_test_raw, y_test_raw) = mnist.load_data()
</pre></div></br>
									
									Next step is create a neural network model in keras! <span style='font-weight:bold'>Keras API</span> is pretty intuitive for you to create a neural network in few lines of code.</br></p>

									<p>
										<div style="background:black;overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;">
										<pre style="margin: 0; line-height: 125%">
 model.add(Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1) ))
 model.add(Conv2D(32, (3, 3), activation='relu'))
 model.add(MaxPooling2D((2, 2)))
 model.add(Conv2D(16, (3, 3), activation='relu'))
 model.add(Flatten())
 model.add(Dense(32, activation='relu'))
 model.add(Dense(total_classes, activation='softmax'))
 model.compile(loss=keras.losses.categorical_crossentropy,
            optimizer=keras.optimizers.Adadelta(),
            metrics=['accuracy'])   
</pre></div></br>
									<span style='font-weight:bold;color:red'>Training on correctly labelled data</span></br>
									Once we have the data and the model, we first train the model with correctly labelled data as loaded from keras datasets. We observe that we are able to achieve 99+% accuracy after just 5 epochs!</p>
								
								<figure>
  								<center><img src="images/project1/graph1.png" alt="Accuracy graph for first model" style='background-color:white;'>
  								<figcaption>Accuracy for model running on correctly labelled data.</figcaption></center></br>
								</figure> 


								<p>
								<span style='font-weight:bold;color:red'>Training on randomly labelled data</span></br>
									Next we try training the model with totally random data and the same model as defined above. For each image in our dataset, we randomly assign it a number! We are able to achieve slightly more than 30% accuracy after 50 iterations, which is very poor! We also see that model is deviating from validation data, this can be explained by the fact that the distribution of training and validation data is different! Maybe the network isn't big enough to be able to learn the patterns in this randomly labelled data!</p>
								<figure>
  								<center><img src="images/project1/graph2.png" alt="Accuracy graph using randomly labelled data" style='background-color:white;'>
								<figcaption>Accuracy for model running on randomly labelled data.</figcaption></center></br>
								</figure> 

								<p>
								<span style='font-weight:bold;color:red'>Training a BIGGER network on randomly labelled data</span></br>
								So we create a bigger neural network! Note that our expectations at this point are to get 99+% accuracy on the training data. Network will perform very poorly on the validation and testing dataset anyways, because the patterns learnt on randomly labelled training dataset are not valid for validation and testing dataset at all! In essence, we expect network to memoize the training data mapping</p>
								<div style="background:black;overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;">
										<pre style="margin: 0; line-height: 125%">
 model = Sequential()
 model.add(Conv2D(128, (3,3), activation='relu', input_shape=(28,28,1) ))
 model.add(Conv2D(128, (3, 3), activation='relu'))
 model.add(MaxPooling2D((2, 2)))
 model.add(Conv2D(128, (3, 3), activation='relu'))
 model.add(Flatten())
 model.add(Dense(128, activation='relu'))
 model.add(Dense(total_classes, activation='softmax'))
 model.compile(loss=keras.losses.categorical_crossentropy,
            optimizer=keras.optimizers.Adadelta(),
            metrics=['accuracy'])
</pre></div></br>

								<p> Well well well...We do see 99+% accuracy on the neural network for randomly labelled data! However given the lack of structure in the data, it takes longer time to find a mapping which essentially seems to be a learning the training data. Network takes about 50 iterations to finally achieve 99+% accuracy on training data. Note that, as expected, validation data accuracy is pretty low! If you ponder a bit, 10% accuracy on validation data is exactly what one would get if it were to choose same digit every single time, this seems to be in line with fact that data was created at random, assigning one of the 10 digits, with each digit's probability being 0.1.</p>
								<figure>
  								<center><img src="images/project1/graph3.png" alt="Accuracy graph using randomly labelled data" style='background-color:white;'>
								<figcaption>Accuracy for BIGGER model running on randomly labelled data.</figcaption></center></br>
								</figure> 
			
								<p style='font-weight:bold'> <span style='color:red'>What do we learn?</span></br>
								Neural networks are all about reducing the difference between predicted mapping and actual mapping of the data it is being TRAINED on. The key is "data it's being trained on". If training data is generalized enough and network is suited to learn those generalized patterns, network will perform well on validation and testing datasets too, as we observe in our very first run of model on correctly labelled MNIST data. However, even if you provide the network with incorrect or randomly labelled data, given enough size and epochs to train, a neural network will achieve good accuracy on that dataset too. However due to lack of structure in the training data, the mapping network learns would not be useful on validation and training datasets!
								</div>
							</section>


					</div>

				<!-- Footer -->
					<footer id="footer" style='background-color:black'>
						<div class="inner">
							<ul class="icons">
								<li><a href="https://www.twitter.com/guru3s" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a></li>
								<li><a href="https://www.instagram.com/guru3s" class="icon brands alt fa-instagram"><span class="label">Instagram</span></a></li>
								<li><a href="https://www.github.com/guru3" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
								<li><a href="https://www.linkedin.com/in/guru3s" class="icon brands alt fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>